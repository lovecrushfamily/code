{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine learning project pipeline\n",
    "\n",
    "How to dive into a new machine learning project? (full guidance myself)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.Tool for machine learning approach\n",
    "\n",
    "- Language: Python\n",
    "- Libs for data modeling: sklearn, xgboost,..\n",
    "- Libs for data analysis: numpy, pandas,..\n",
    "- Libs for data visualization: matplotlib, seaborn,..\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "\n",
    "from sklearn.tree import DecisionTreeRegressor, DecisionTreeClassifier, plot_tree\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.ensemble import GradientBoostingClassifier, GradientBoostingRegressor  # deprecated\n",
    "from sklearn.ensemble import AdaBoostClassifier, AdaBoostRegressor\n",
    "from sklearn.ensemble import VotingClassifier, VotingRegressor\n",
    "from sklearn.ensemble import BaggingClassifier, BaggingRegressor\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LinearRegression,LogisticRegression, Lasso, Ridge\n",
    "from sklearn.naive_bayes import GaussianNB, BernoulliNB\n",
    "from sklearn.svm import SVC, SVR\n",
    "from sklearn.mixture import GaussianMixture, BayesianGaussianMixture\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder, LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler, Normalizer, MinMaxScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import MDS, LocallyLinearEmbedding, Isomap, TSNE\n",
    "from sklearn.cluster import KMeans, SpectralClustering  , MiniBatchKMeans\n",
    "from sklearn.impute import SimpleImputer, KNNImputer\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold, StratifiedGroupKFold, StratifiedShuffleSplit\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "\n",
    "from sklearn.model_selection import train_test_split, learning_curve\n",
    "from sklearn.model_selection import cross_val_score, cross_validate, cross_val_predict\n",
    "from sklearn.metrics import accuracy_score, f1_score, mean_absolute_error, mean_squared_error\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier, MLPRegressor\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "\n",
    "# from xgboost import XGBClassifier, XGBRegressor\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.pipeline import make_pipeline, Pipeline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's a great collection of scikit-learn tools you've imported! Here's a breakdown of some commonly used ones, along with their applications, data suitability, pros, and cons:\n",
    "\n",
    "**1. Classification Algorithms:**\n",
    "\n",
    "* **DecisionTreeClassifier:**\n",
    "    * **When to use:** Good for interpretable models, handling mixed-data types, and identifying important features.\n",
    "    * **Data:** Categorical or numerical data.\n",
    "    * **Pros:** Interpretable, robust to outliers.\n",
    "    * **Cons:** Prone to overfitting, high variance.\n",
    "\n",
    "* **RandomForestClassifier:**\n",
    "    * **When to use:** Ensemble of decision trees for improved accuracy and reduced variance.\n",
    "    * **Data:** Categorical or numerical data.\n",
    "    * **Pros:** More accurate than single decision trees, handles mixed data types.\n",
    "    * **Cons:** Less interpretable than single decision trees.\n",
    "\n",
    "* **KNeighborsClassifier:**\n",
    "    * **When to use:** Good for classification tasks with well-defined clusters.\n",
    "    * **Data:** Numerical data.\n",
    "    * **Pros:** Simple, effective for certain datasets.\n",
    "    * **Cons:** High computational cost for large datasets, sensitive to outliers and curse of dimensionality.\n",
    "\n",
    "* **LogisticRegression:**\n",
    "    * **When to use:** Classifies data into binary classes (0 or 1).\n",
    "    * **Data:** Numerical data (often with feature scaling).\n",
    "    * **Pros:** Interpretable coefficients, good for binary classification problems.\n",
    "    * **Cons:** Limited to binary classification, may not capture complex relationships.\n",
    "\n",
    "* **GaussianNB:**\n",
    "    * **When to use:** Efficient for large datasets, works well with naive Bayes assumptions.\n",
    "    * **Data:** Numerical data (often with feature scaling).\n",
    "    * **Pros:** Fast, works well with certain assumptions.\n",
    "    * **Cons:** Assumes independence of features, may not be suitable for complex relationships.\n",
    "\n",
    "* **SVC (Support Vector Classifier):**\n",
    "    * **When to use:** Powerful for high-dimensional data, good for separating classes with a clear margin.\n",
    "    * **Data:** Numerical data (often with feature scaling).\n",
    "    * **Pros:** Effective for high-dimensional data, handles outliers well.\n",
    "    * **Cons:** Can be computationally expensive, less interpretable.\n",
    "\n",
    "* **XGBClassifier:**\n",
    "    * **When to use:** Powerful ensemble method, often outperforms other algorithms.\n",
    "    * **Data:** Categorical or numerical data.\n",
    "    * **Pros:** Highly accurate, handles various data types, offers regularization.\n",
    "    * **Cons:** Can be computationally expensive, less interpretable than some algorithms.\n",
    "\n",
    "**2. Regression Algorithms:**\n",
    "\n",
    "* **DecisionTreeRegressor:**\n",
    "    * **When to use:** Similar to DecisionTreeClassifier for regression tasks.\n",
    "    * **Data:** Categorical or numerical data.\n",
    "    * **Pros:** Interpretable, good for handling missing values.\n",
    "    * **Cons:** Prone to overfitting, high variance.\n",
    "\n",
    "* **RandomForestRegressor:**\n",
    "    * **When to use:** Ensemble of decision trees for improved accuracy and reduced variance in regression.\n",
    "    * **Data:** Categorical or numerical data.\n",
    "    * **Pros:** More accurate than single decision trees, handles mixed data types.\n",
    "    * **Cons:** Less interpretable than single decision trees.\n",
    "\n",
    "* **KNeighborsRegressor:**\n",
    "    * **When to use:** Similar to KNeighborsClassifier for regression tasks.\n",
    "    * **Data:** Numerical data.\n",
    "    * **Pros:** Simple, effective for certain datasets.\n",
    "    * **Cons:** High computational cost for large datasets, sensitive to outliers and curse of dimensionality.\n",
    "\n",
    "* **LinearRegression:**\n",
    "    * **When to use:** Models linear relationships between features and target variable.\n",
    "    * **Data:** Numerical data (often with feature scaling).\n",
    "    * **Pros:** Interpretable coefficients, simple to understand.\n",
    "    * **Cons:** Assumes linear relationships, may not be suitable for complex relationships.\n",
    "\n",
    "* **MLPRegressor (Multi-layer Perceptron):**\n",
    "    * **When to use:** Powerful for non-linear relationships, can learn complex patterns.\n",
    "    * **Data:** Numerical data.\n",
    "    * **Pros:** Handles non-linear relationships, flexible architecture.\n",
    "    * **Cons:** Prone to overfitting, requires careful hyperparameter tuning.\n",
    "\n",
    "* **XGBRegressor:**\n",
    "    * **When to use:** Ensemble method often outperforming other algorithms for regression.\n",
    "    * **Data:** Categorical or numerical data.\n",
    "    * **Pros:** Highly accurate, handles various data types, offers regularization.\n",
    "    * **Cons:** Can be computationally expensive, less interpretable than some algorithms.\n",
    "\n",
    "\n",
    "**3. Preprocessing Tools (continued):**\n",
    "\n",
    "* **OneHotEncoder:**\n",
    "    * **Pros:** Efficient for high-cardinality categorical features, avoids introducing artificial ordering.\n",
    "    * **Cons:** Increases feature dimensionality.\n",
    "\n",
    "* **OrdinalEncoder:**\n",
    "    * **Pros:** Preserves some ordinal information of categorical features.\n",
    "    * **Cons:** Assumes a natural ordering exists for categories, may not be suitable for all categorical data.\n",
    "\n",
    "* **LabelEncoder:**\n",
    "    * **Pros:** Simple and efficient for encoding categorical features.\n",
    "    * **Cons:** Introduces artificial ordering, may lead to misinterpretations if categories don't have inherent order.\n",
    "\n",
    "* **PCA (Principal Component Analysis):**\n",
    "    * **When to use:** Reduce dimensionality of data while preserving most of the variance.\n",
    "    * **Data:** Numerical data.\n",
    "    * **Pros:** Reduces computation time and storage requirements, can help visualize high-dimensional data.\n",
    "    * **Cons:** May lose some information during dimensionality reduction.\n",
    "\n",
    "**4. Other Important Tools:**\n",
    "\n",
    "* **KMeans:** Unsupervised clustering algorithm for grouping similar data points. \n",
    "* **SimpleImputer:** Handles missing values in datasets.\n",
    "* **StratifiedKFold, StratifiedGroupKFold, StratifiedShuffleSplit:** Cross-validation techniques for imbalanced classification problems.\n",
    "* **GridSearchCV, RandomizedSearchCV:** Hyperparameter tuning techniques for optimizing model performance.\n",
    "* **Pipeline:** Combines data preprocessing and modeling steps into a single workflow.\n",
    "* **ColumnTransformer:** Applies different preprocessing techniques to different columns.\n",
    "\n",
    "**Choosing the Right Tool:**\n",
    "\n",
    "The selection of the best tool depends on several factors:\n",
    "\n",
    "* **Problem Type:** Classification, regression, clustering, dimensionality reduction, etc.\n",
    "* **Data Type:** Numerical, categorical, mixed data.\n",
    "* **Data Characteristics:** Linearity, relationships between features, presence of outliers, etc.\n",
    "* **Desired Model Properties:** Interpretability, accuracy, efficiency, etc.\n",
    "\n",
    "**General Tips:**\n",
    "\n",
    "* Start with simpler models and explore more complex ones if needed.\n",
    "* Experiment with different algorithms and preprocessing techniques.\n",
    "* Use cross-validation to evaluate model performance on unseen data.\n",
    "* Consider the trade-off between accuracy and interpretability.\n",
    "\n",
    "\n",
    "By understanding these tools and their use cases, you can effectively build and improve your machine learning models using scikit-learn.\n",
    "\n",
    "Do you have any specific questions about a particular tool or a machine learning task you'd like to tackle? I'm happy to provide further guidance!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Dataset for ml approach\n",
    "\n",
    "- Crawl data from internet\n",
    "- Given data\n",
    "- create new data pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = 'independent variable, features'\n",
    "y = 'target, dependent variable    '"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Understand the limit of data \n",
    "\n",
    "- Understand the business problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Analysis data (Basic - not edit, modify anything)\n",
    "\n",
    "- EDA : Explore Data Analysis\n",
    "- Basic statistic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Visualize data\n",
    "\n",
    "- Data correlation\n",
    "- Data distribution\n",
    "- Data missing\n",
    "- Correlation matrix\n",
    "- "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Splitting data before preprocessing data\n",
    "- Avoid data leakage by leaking the test data to the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_holdout, y_train, y_holdout = train_test_split(X, y, random_state=0, test_size=0.3)\n",
    "X_val, X_val, y_test, y_test = train_test_split(X_holdout, y_holdout, test_size=0.5, random_state=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1. Visualize data again and see the differences\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Split data\n",
    "\n",
    "- train test split\n",
    "- split the data after we handling missing data, do not trying to reverse the process\n",
    "- Stratified Kfold\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Data processing\n",
    "\n",
    "- EDA - advance\n",
    "- Data missing processing\n",
    "- feature engineering\n",
    "- Feature selection\n",
    "- StandardScalar features, Normalize, Standardize\n",
    "- Handling missing data, imputation technique, encoding technique, one-hot, ordinal\n",
    "- create new feature,\n",
    "- optimize feature for models,\n",
    "- data wrangling.\n",
    "- Getting everything related to the data at this point, 'cause we focus on hyperparameter later.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Modeling data (baseline model)\n",
    "\n",
    "- Base on experience trying apply pre-built ml models as much as possible on the raw data, \n",
    "- Basic selection\n",
    "- Evaluate models without hyperparameter\n",
    "- Modeling data without hesitated, not require performing complex technique on data this turn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Feature Engineering\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Evaluate vs visualize the model performance\n",
    "\n",
    "- Visualize model performances\n",
    "- Monitor model metrics for tuning later\n",
    "- Plotting the model metrics with params for better picturing\n",
    "- Analyze those curve and make the decision, the next move."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Advance - Fine-tuning mode, hyperparameter tuning\n",
    "\n",
    "- Trying apply all skillset on the model, \n",
    "- cross-validation\n",
    "- confusion metric\n",
    "- keep tracking the model metrics for be better move\n",
    "- "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Keep hyperparameter model until model can be able to capture the patterns of data\n",
    "\n",
    "- repeating the processing, evaluate -> validation -> visualize -> evaluate\n",
    "- keep doing until everything's gonna touch its limits."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 10. Pipeline\n",
    "- On how to combine every process in one fit line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # model_rf = \n",
    "\n",
    "# pipeline = make_pipeline([\n",
    "#     OneHotEncoder(handle_unknown='ignore'),\n",
    "#     OrdinalEncoder(),\n",
    "#     RandomForestClassifier(random_state=100, n_estimators=100),\n",
    "#     XGBClassifier(n_estimators=100),\n",
    "#     KNeighborsClassifier(),\n",
    "#     GaussianNB()\n",
    "    \n",
    "    \n",
    "#     # XGBRegressor(n_estimators=100),\n",
    "\n",
    "# ],verbose=True)\n",
    "# pipeline.steps\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "string index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 23\u001b[0m\n\u001b[1;32m     19\u001b[0m             nee_pointer\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m---> 23\u001b[0m \u001b[43mstrStr\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msadbutsad\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msad\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[3], line 11\u001b[0m, in \u001b[0;36mstrStr\u001b[0;34m(haystack, needle)\u001b[0m\n\u001b[1;32m      7\u001b[0m limits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(needle)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m hay_pointer \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m max_iters:\n\u001b[0;32m---> 11\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m haystack[hay_pointer] \u001b[38;5;241m!=\u001b[39m \u001b[43mneedle\u001b[49m\u001b[43m[\u001b[49m\u001b[43mnee_pointer\u001b[49m\u001b[43m]\u001b[49m:\n\u001b[1;32m     12\u001b[0m         hay_pointer\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     14\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m nee_pointer \u001b[38;5;241m==\u001b[39m limits:\n",
      "\u001b[0;31mIndexError\u001b[0m: string index out of range"
     ]
    }
   ],
   "source": [
    "def strStr(haystack: str, needle: str) -> int:\n",
    "\n",
    "    hay_pointer = 0\n",
    "    nee_pointer = 0\n",
    "    temp_index = 0\n",
    "    max_iters = len(haystack)\n",
    "    limits = len(needle)\n",
    "\n",
    "    while hay_pointer <= max_iters :\n",
    "\n",
    "        if haystack[hay_pointer] != needle[nee_pointer]:\n",
    "            hay_pointer+=1\n",
    "\n",
    "            if nee_pointer == limits:\n",
    "                return hay_pointer - limits\n",
    "        \n",
    "        elif haystack[hay_pointer] == needle[nee_pointer]:\n",
    "            hay_pointer+=1\n",
    "            nee_pointer+=1\n",
    "\n",
    "    return False\n",
    "\n",
    "strStr(\"sadbutsad\", \"sad\")\n",
    "\n",
    "        \n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
